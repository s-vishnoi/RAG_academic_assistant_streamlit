# Local RAG Research Assistant 
It's winter in Chicago and I'm too lazy to finish my readings and finish all my research projects in time. I am 
deploying a personal academic assitant which can summarize my research documents on my local system i.e. with complete research privacy, free, and fine-tunable. 

This project is built on:
- *LangChain*: integration of document loaders, vector stores, and LLMs.
- *DeepSeek-R1*: open-source reasoning LLM.
- *Ollama*: A CLI tool for managing local LLMs and embedding models. 
- *ChromaDB*: vector database to store and retrieve document embeddings for similarity-based queries.
- *Streamlit*: Web interface for RAG application. 

The final RAG architecture will be: 

<img width="674" alt="Screenshot 2025-02-11 at 4 15 00 PM" src="https://github.com/user-attachments/assets/b0f17b43-f395-40bc-99d7-f9d47be52e94" /> [1]



References:
[1] https://blog.gopenai.com/how-to-build-a-privacy-first-rag-using-deepseek-r1-langchain-and-ollama-c5133a8514dd
